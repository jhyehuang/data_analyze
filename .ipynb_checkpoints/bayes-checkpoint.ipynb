{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "\n",
    "def loadDataSet():\n",
    "    postingList=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "                 ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                 ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                 ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                 ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    classVec = [0,1,0,1,0,1]    #1 is abusive, 0 not\n",
    "    return postingList,classVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createVocabList(dataSet):\n",
    "    vocabSet = set([])  #create empty set\n",
    "    for document in dataSet:\n",
    "        vocabSet = vocabSet | set(document) #union of the two sets\n",
    "    return list(vocabSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else: print (\"the word: %s is not in my Vocabulary!\" % word)\n",
    "    return returnVec\n",
    "\n",
    "def bagOfWords2VecMN(vocabList, inputSet):\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] += 1\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "listOPosts,listClasses=loadDataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 0, 1, 0, 1]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listClasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myVocabList=createVocabList(listOPosts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setOfWords2Vec(myVocabList,listOPosts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainNB0(trainMatrix,trainCategory):\n",
    "    numTrainDocs = len(trainMatrix)   #几个文档\n",
    "    numWords = len(trainMatrix[0])    #文档中有几个单词\n",
    "    pAbusive = sum(trainCategory)/float(numTrainDocs)\n",
    "    p0Num = ones(numWords); p1Num = ones(numWords)      #change to ones() \n",
    "    p0Denom = 2.0; p1Denom = 2.0                        #change to 2.0\n",
    "    for i in range(numTrainDocs):\n",
    "        if trainCategory[i] == 1:\n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    p1Vect = log(p1Num/p1Denom)          #change to log()\n",
    "    p0Vect = log(p0Num/p0Denom)          #change to log()\n",
    "    return p0Vect,p1Vect,pAbusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainMat=[]\n",
    "for postinDoc in listOPosts:\n",
    "    trainMat.append(setOfWords2Vec(myVocabList,postinDoc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainMat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p0V,p1V,pAb=trainNB0(trainMat,listClasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-2.15948425, -2.56494936, -2.56494936, -2.56494936, -2.56494936,\n",
       "        -1.87180218, -3.25809654, -3.25809654, -2.56494936, -2.56494936,\n",
       "        -2.56494936, -3.25809654, -2.56494936, -3.25809654, -2.56494936,\n",
       "        -2.56494936, -2.56494936, -3.25809654, -2.56494936, -3.25809654,\n",
       "        -3.25809654, -2.56494936, -2.56494936, -2.56494936, -3.25809654,\n",
       "        -2.56494936, -2.56494936, -2.56494936, -3.25809654, -3.25809654,\n",
       "        -3.25809654, -2.56494936]),\n",
       " array([-2.35137526, -3.04452244, -3.04452244, -3.04452244, -3.04452244,\n",
       "        -3.04452244, -2.35137526, -2.35137526, -3.04452244, -3.04452244,\n",
       "        -3.04452244, -2.35137526, -3.04452244, -1.94591015, -3.04452244,\n",
       "        -1.94591015, -3.04452244, -2.35137526, -3.04452244, -1.65822808,\n",
       "        -2.35137526, -2.35137526, -3.04452244, -3.04452244, -2.35137526,\n",
       "        -3.04452244, -3.04452244, -2.35137526, -2.35137526, -2.35137526,\n",
       "        -2.35137526, -3.04452244]),\n",
       " 0.5)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p0V,p1V,pAb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    p1 = sum(vec2Classify * p1Vec) + log(pClass1)    #element-wise mult\n",
    "    p0 = sum(vec2Classify * p0Vec) + log(1.0 - pClass1)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else: \n",
    "        return 0\n",
    "\n",
    "def testingNB():\n",
    "    listOPosts,listClasses = loadDataSet()\n",
    "    myVocabList = createVocabList(listOPosts)\n",
    "    trainMat=[]\n",
    "    for postinDoc in listOPosts:\n",
    "        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "    p0V,p1V,pAb = trainNB0(array(trainMat),array(listClasses))\n",
    "    testEntry = ['love', 'my', 'dalmation']\n",
    "    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print (testEntry,'classified as: ',classifyNB(thisDoc,p0V,p1V,pAb))\n",
    "    testEntry = ['stupid', 'garbage']\n",
    "    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print (testEntry,'classified as: ',classifyNB(thisDoc,p0V,p1V,pAb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'my', 'dalmation'] classified as:  0\n",
      "['stupid', 'garbage'] classified as:  1\n"
     ]
    }
   ],
   "source": [
    "testingNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def textParse(bigString):    #input is big string, #output is word list\n",
    "    import re\n",
    "    listOfTokens = re.split('\\W*', bigString)\n",
    "    return [tok.lower() for tok in listOfTokens if len(tok) > 2] \n",
    "    \n",
    "def coding_detect(filename):\n",
    "    #bytes = min(32, os.path.getsize(filename))\n",
    "    #raw = open(filename, 'rb').read(bytes)\n",
    "    #result = chardet.detect(raw)\n",
    "    #encoding = result['encoding']\n",
    "    encoding='unicode-escape'\n",
    "    return encoding\n",
    "\n",
    "def spamTest():\n",
    "    import os\n",
    "    import chardet\n",
    "    docList=[]; classList = []; fullText =[]\n",
    "    for i in range(1,26):\n",
    "        filename_spam='machinelearninginaction\\\\Ch04\\\\email\\\\spam\\\\%d.txt' % i\n",
    "        encoding=coding_detect(filename_spam)\n",
    "        filestr=open(filename_spam,'r',encoding = encoding).read()\n",
    "        wordList = textParse(filestr)\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)   #求并集\n",
    "        classList.append(1)\n",
    "        filename_ham='machinelearninginaction\\\\Ch04\\\\email\\\\ham\\\\%d.txt' % i\n",
    "        encoding=coding_detect(filename_ham)\n",
    "        filestr=open(filename_ham,'r',encoding = encoding).read()\n",
    "        wordList = textParse(filestr)\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(0)\n",
    "    vocabList = createVocabList(docList)#create vocabulary  全部单词的字典\n",
    "    trainingSet = list(range(50)); testSet=[]           #create test set\n",
    "    for i in range(10):\n",
    "        randIndex = int(random.uniform(0,len(trainingSet)))\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        del(trainingSet[randIndex])  \n",
    "    trainMat=[]; trainClasses = []\n",
    "    for docIndex in trainingSet:#train the classifier (get probs) trainNB0\n",
    "        trainMat.append(bagOfWords2VecMN(vocabList, docList[docIndex]))\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    p0V,p1V,pSpam = trainNB0(array(trainMat),array(trainClasses))\n",
    "    errorCount = 0\n",
    "    for docIndex in testSet:        #classify the remaining items\n",
    "        wordVector = bagOfWords2VecMN(vocabList, docList[docIndex])\n",
    "        if classifyNB(array(wordVector),p0V,p1V,pSpam) != classList[docIndex]:\n",
    "            errorCount += 1\n",
    "            print (\"classification error\",docList[docIndex])\n",
    "    print ('the error rate is: ',float(errorCount)/len(testSet))\n",
    "    #return vocabList,fullText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "the error rate is:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "spamTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcMostFreq(vocabList,fullText):\n",
    "    import operator\n",
    "    freqDict = {}\n",
    "    for token in vocabList:\n",
    "        freqDict[token]=fullText.count(token)\n",
    "    sortedFreq = sorted(freqDict.items(), key=operator.itemgetter(1), reverse=True) \n",
    "    return sortedFreq[:30]\n",
    "\n",
    "def localWords(feed1,feed0):\n",
    "    import feedparser\n",
    "    docList=[]; classList = []; fullText =[]\n",
    "    minLen = min(len(feed1['entries']),len(feed0['entries']))\n",
    "    for i in range(minLen):\n",
    "        wordList = textParse(feed1['entries'][i]['summary'])\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(1) #NY is class 1\n",
    "        wordList = textParse(feed0['entries'][i]['summary'])\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(0)\n",
    "    vocabList = createVocabList(docList)#create vocabulary\n",
    "    top30Words = calcMostFreq(vocabList,fullText)   #remove top 30 words\n",
    "    for pairW in top30Words:\n",
    "        if pairW[0] in vocabList: vocabList.remove(pairW[0])\n",
    "    trainingSet = list(range(2*minLen)); testSet=[]           #create test set\n",
    "    for i in range(20):\n",
    "        randIndex = int(random.uniform(0,len(trainingSet)))\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        del(trainingSet[randIndex])  \n",
    "    trainMat=[]; trainClasses = []\n",
    "    for docIndex in trainingSet:#train the classifier (get probs) trainNB0\n",
    "        trainMat.append(bagOfWords2VecMN(vocabList, docList[docIndex]))\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    p0V,p1V,pSpam = trainNB0(array(trainMat),array(trainClasses))\n",
    "    errorCount = 0\n",
    "    for docIndex in testSet:        #classify the remaining items\n",
    "        wordVector = bagOfWords2VecMN(vocabList, docList[docIndex])\n",
    "        if classifyNB(array(wordVector),p0V,p1V,pSpam) != classList[docIndex]:\n",
    "            errorCount += 1\n",
    "    print ('the error rate is: ',float(errorCount)/len(testSet))\n",
    "    return vocabList,p0V,p1V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error rate is:  0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "import feedparser\n",
    "ny=feedparser.parse('http://newyork.craigslist.org/stp/index.rss')\n",
    "sf=feedparser.parse('http://sfbay.craigslist.org/stp/index.rss')\n",
    "vocabList,pSF,pNY=localWords(ny,sf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTopWords(ny,sf):\n",
    "    import operator\n",
    "    vocabList,p0V,p1V=localWords(ny,sf)\n",
    "    topNY=[]; topSF=[]\n",
    "    for i in range(len(p0V)):\n",
    "        if p0V[i] > -6.0 : topSF.append((vocabList[i],p0V[i]))\n",
    "        if p1V[i] > -6.0 : topNY.append((vocabList[i],p1V[i]))\n",
    "    sortedSF = sorted(topSF, key=lambda pair: pair[1], reverse=True)\n",
    "    print (\"SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**\")\n",
    "    for item in sortedSF:\n",
    "        print (item[0])\n",
    "    sortedNY = sorted(topNY, key=lambda pair: pair[1], reverse=True)\n",
    "    print (\"NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**\")\n",
    "    for item in sortedNY:\n",
    "        print (item[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error rate is:  0.45\n",
      "SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**\n",
      "respectful\n",
      "down\n",
      "going\n",
      "contact\n",
      "year\n",
      "naked\n",
      "hello\n",
      "meeting\n",
      "chat\n",
      "age\n",
      "exchange\n",
      "over\n",
      "coffee\n",
      "thursday\n",
      "happy\n",
      "small\n",
      "games\n",
      "ride\n",
      "kind\n",
      "open\n",
      "started\n",
      "her\n",
      "ddf\n",
      "yourself\n",
      "your\n",
      "well\n",
      "seeking\n",
      "good\n",
      "please\n",
      "many\n",
      "sane\n",
      "all\n",
      "ideas\n",
      "used\n",
      "share\n",
      "line\n",
      "from\n",
      "best\n",
      "been\n",
      "hiking\n",
      "join\n",
      "hang\n",
      "movies\n",
      "buddy\n",
      "tubs\n",
      "affection\n",
      "shook\n",
      "worries\n",
      "where\n",
      "pained\n",
      "anything\n",
      "foolishness\n",
      "healer\n",
      "talking\n",
      "visiting\n",
      "mentally\n",
      "helpful\n",
      "alone\n",
      "younger\n",
      "fun\n",
      "hoping\n",
      "interested\n",
      "concert\n",
      "still\n",
      "questions\n",
      "earth\n",
      "don\n",
      "back\n",
      "having\n",
      "glass\n",
      "nearby\n",
      "tree\n",
      "distress\n",
      "anyone\n",
      "work\n",
      "text\n",
      "communication\n",
      "motivation\n",
      "parkinson\n",
      "sincere\n",
      "lighter\n",
      "straight\n",
      "today\n",
      "gather\n",
      "music\n",
      "assets\n",
      "swm\n",
      "occasion\n",
      "title\n",
      "hanging\n",
      "travel\n",
      "incidents\n",
      "people\n",
      "conversationalist\n",
      "try\n",
      "subject\n",
      "getting\n",
      "room\n",
      "give\n",
      "tall\n",
      "route\n",
      "come\n",
      "hardships\n",
      "drop\n",
      "about\n",
      "activities\n",
      "tell\n",
      "married\n",
      "will\n",
      "woman\n",
      "strip\n",
      "conversation\n",
      "friends\n",
      "three\n",
      "lift\n",
      "hot\n",
      "how\n",
      "lie\n",
      "each\n",
      "convenient\n",
      "cultivate\n",
      "lets\n",
      "hold\n",
      "now\n",
      "creative\n",
      "somebody\n",
      "advise\n",
      "yes\n",
      "per\n",
      "makes\n",
      "past\n",
      "foot\n",
      "hey\n",
      "routine\n",
      "did\n",
      "glen\n",
      "ring\n",
      "hard\n",
      "know\n",
      "emotionally\n",
      "variety\n",
      "again\n",
      "cannabis\n",
      "attractive\n",
      "weekend\n",
      "lately\n",
      "six\n",
      "possible\n",
      "tendril\n",
      "got\n",
      "thought\n",
      "own\n",
      "email\n",
      "enjoy\n",
      "smart\n",
      "hear\n",
      "town\n",
      "adventurous\n",
      "guy\n",
      "healing\n",
      "topics\n",
      "entry\n",
      "lean\n",
      "other\n",
      "buddies\n",
      "otherwise\n",
      "interest\n",
      "mutually\n",
      "forgive\n",
      "normal\n",
      "wine\n",
      "stress\n",
      "muscular\n",
      "difference\n",
      "monty\n",
      "company\n",
      "before\n",
      "bike\n",
      "two\n",
      "thinking\n",
      "perhaps\n",
      "forward\n",
      "says\n",
      "way\n",
      "hotel\n",
      "times\n",
      "socialize\n",
      "compensate\n",
      "gone\n",
      "rave\n",
      "tennis\n",
      "home\n",
      "body\n",
      "nature\n",
      "recently\n",
      "physically\n",
      "passions\n",
      "non\n",
      "was\n",
      "shopping\n",
      "ready\n",
      "hurt\n",
      "means\n",
      "walking\n",
      "find\n",
      "nights\n",
      "willow\n",
      "sweat\n",
      "drive\n",
      "maybe\n",
      "edm\n",
      "business\n",
      "food\n",
      "longer\n",
      "medical\n",
      "they\n",
      "tensions\n",
      "relax\n",
      "night\n",
      "lunch\n",
      "arts\n",
      "plants\n",
      "excited\n",
      "cool\n",
      "these\n",
      "reply\n",
      "really\n",
      "power\n",
      "youre\n",
      "make\n",
      "most\n",
      "female\n",
      "posting\n",
      "soak\n",
      "movie\n",
      "could\n",
      "interesting\n",
      "clothed\n",
      "involved\n",
      "gentleman\n",
      "shape\n",
      "nice\n",
      "answered\n",
      "bimwm\n",
      "etc\n",
      "didn\n",
      "clean\n",
      "man\n",
      "lot\n",
      "friendly\n",
      "cute\n",
      "executive\n",
      "toned\n",
      "flaky\n",
      "intelligent\n",
      "little\n",
      "scale\n",
      "edc\n",
      "women\n",
      "dont\n",
      "wide\n",
      "live\n",
      "fee\n",
      "using\n",
      "clear\n",
      "full\n",
      "pay\n",
      "hug\n",
      "girl\n",
      "long\n",
      "ever\n",
      "treatment\n",
      "possibly\n",
      "club\n",
      "55lbs\n",
      "stuff\n",
      "flash\n",
      "break\n",
      "honest\n",
      "brainchild\n",
      "ordained\n",
      "turns\n",
      "usually\n",
      "exotic\n",
      "girls\n",
      "relationships\n",
      "anastasia\n",
      "hands\n",
      "young\n",
      "light\n",
      "put\n",
      "offer\n",
      "often\n",
      "6989\n",
      "917\n",
      "yours\n",
      "intended\n",
      "asian\n",
      "its\n",
      "lighting\n",
      "events\n",
      "cam\n",
      "early\n",
      "let\n",
      "gay\n",
      "days\n",
      "pleasurable\n",
      "modeling\n",
      "judge\n",
      "clip\n",
      "much\n",
      "tangible\n",
      "clo\n",
      "had\n",
      "keeping\n",
      "deal\n",
      "hackney\n",
      "trying\n",
      "expectations\n",
      "sexy\n",
      "aussiebum\n",
      "fall\n",
      "friend\n",
      "sched\n",
      "race\n",
      "always\n",
      "lots\n",
      "missing\n",
      "mid\n",
      "years\n",
      "1967\n",
      "backgrounds\n",
      "any\n",
      "working\n",
      "japanese\n",
      "upon\n",
      "monterey\n",
      "suck\n",
      "comfortable\n",
      "pics\n",
      "those\n",
      "place\n",
      "cuddle\n",
      "parlor\n",
      "dresses\n",
      "situation\n",
      "head\n",
      "doing\n",
      "spent\n",
      "person\n",
      "includes\n",
      "0553\n",
      "pic\n",
      "professional\n",
      "dressed\n",
      "student\n",
      "point\n",
      "one\n",
      "dim\n",
      "middle\n",
      "fit\n",
      "accountant\n",
      "big\n",
      "ages\n",
      "paul\n",
      "religion\n",
      "cuddler\n",
      "becomes\n",
      "freaky\n",
      "college\n",
      "take\n",
      "oil\n",
      "host\n",
      "distance\n",
      "functional\n",
      "underwear\n",
      "something\n",
      "everyone\n",
      "random\n",
      "learning\n",
      "552\n",
      "held\n",
      "articles\n",
      "xoxo\n",
      "else\n",
      "tissue\n",
      "messages\n",
      "also\n",
      "date\n",
      "moments\n",
      "seeing\n",
      "took\n",
      "easy\n",
      "gym\n",
      "pretty\n",
      "life\n",
      "putting\n",
      "check\n",
      "different\n",
      "everything\n",
      "split\n",
      "play\n",
      "recent\n",
      "schedule\n",
      "towels\n",
      "broke\n",
      "relate\n",
      "undies\n",
      "sucks\n",
      "real\n",
      "minister\n",
      "size\n",
      "therapist\n",
      "hangups\n",
      "week\n",
      "discreet\n",
      "touches\n",
      "offended\n",
      "felt\n",
      "relationship\n",
      "together\n",
      "bit\n",
      "special\n",
      "generous\n",
      "myday\n",
      "banana\n",
      "guys\n",
      "image\n",
      "touch\n",
      "brk\n",
      "employed\n",
      "netflix\n",
      "sensual\n",
      "dinner\n",
      "surmount\n",
      "mood\n",
      "stimulation\n",
      "california\n",
      "section\n",
      "first\n",
      "wondering\n",
      "rosa\n",
      "feel\n",
      "unhurried\n",
      "lips\n",
      "420\n",
      "during\n",
      "thing\n",
      "fellas\n",
      "named\n",
      "jazmine\n",
      "enjoys\n",
      "718\n",
      "craigslist\n",
      "361\n",
      "latinos\n",
      "plz\n",
      "gent\n",
      "newyork\n",
      "because\n",
      "black\n",
      "care\n",
      "wants\n",
      "serious\n",
      "laid\n",
      "sure\n",
      "chill\n",
      "odds\n",
      "towards\n",
      "connectedness\n",
      "weekday\n",
      "massage\n",
      "note\n",
      "cut\n",
      "party\n",
      "suspect\n",
      "10am\n",
      "pony\n",
      "attached\n",
      "busy\n",
      "diamond\n",
      "com\n",
      "hangout\n",
      "fabric\n",
      "massages\n",
      "hits\n",
      "winter\n",
      "bikini\n",
      "trim\n",
      "watch\n",
      "great\n",
      "takes\n",
      "offered\n",
      "chemistry\n",
      "relaxing\n",
      "hire\n",
      "time\n",
      "off\n",
      "hair\n",
      "train\n",
      "write\n",
      "choi\n",
      "underground\n",
      "indefinite\n",
      "fine\n",
      "more\n",
      "available\n",
      "things\n",
      "welcome\n",
      "dinners\n",
      "successfully\n",
      "partners\n",
      "regular\n",
      "athletic\n",
      "briefs\n",
      "canal\n",
      "bed\n",
      "few\n",
      "simultaneously\n",
      "crazy\n",
      "experience\n",
      "m4w\n",
      "watching\n",
      "sink\n",
      "persona\n",
      "fitness\n",
      "apply\n",
      "under\n",
      "private\n",
      "campos\n",
      "match\n",
      "then\n",
      "exact\n",
      "showing\n",
      "emotional\n",
      "fwb\n",
      "dates\n",
      "against\n",
      "expected\n",
      "old\n",
      "picture\n",
      "https\n",
      "fox\n",
      "area\n",
      "jealous\n",
      "cup\n",
      "personal\n",
      "puertoriceña\n",
      "strictly\n",
      "planet\n",
      "nyc\n",
      "slender\n",
      "has\n",
      "6pm\n",
      "dancing\n",
      "sex\n",
      "magazine\n",
      "pictures\n",
      "members\n",
      "separate\n",
      "storm\n",
      "rather\n",
      "nipples\n",
      "dude\n",
      "convinced\n",
      "passionate\n",
      "prostate\n",
      "what\n",
      "midtown\n",
      "our\n",
      "psychotherapy\n",
      "baby\n",
      "extra\n",
      "feminine\n",
      "payments\n",
      "workout\n",
      "strong\n",
      "dismantle\n",
      "after\n",
      "curse\n",
      "guess\n",
      "into\n",
      "cuddling\n",
      "luv\n",
      "drink\n",
      "mon\n",
      "m4mw\n",
      "m4m\n",
      "puerto\n",
      "mare\n",
      "euph\n",
      "final\n",
      "easier\n",
      "platonic\n",
      "smoke\n",
      "cash\n",
      "baths\n",
      "elegant\n",
      "weekends\n",
      "amp\n",
      "peeking\n",
      "marry\n",
      "similar\n",
      "smooth\n",
      "send\n",
      "org\n",
      "happens\n",
      "ask\n",
      "sacramento\n",
      "dot\n",
      "his\n",
      "enhance\n",
      "even\n",
      "free\n",
      "shower\n",
      "nothingness\n",
      "name\n",
      "deep\n",
      "trini\n",
      "session\n",
      "queens\n",
      "bounce\n",
      "model\n",
      "last\n",
      "hugs\n",
      "horrible\n",
      "eastern\n",
      "minimum\n",
      "skiers\n",
      "replies\n",
      "sub\n",
      "breast\n",
      "able\n",
      "explore\n",
      "mentee\n",
      "lotion\n",
      "day\n",
      "around\n",
      "contac\n",
      "website\n",
      "possessiv\n",
      "silvia\n",
      "vers\n",
      "kiss\n",
      "myself\n",
      "relaxation\n",
      "soft\n",
      "type\n",
      "money\n",
      "silky\n",
      "kik\n",
      "legit\n",
      "hopefully\n",
      "radical\n",
      "html\n",
      "bindrim\n",
      "frie\n",
      "confused\n",
      "subwrestlingclub\n",
      "specific\n",
      "heated\n",
      "bodywork\n",
      "advice\n",
      "looks\n",
      "them\n",
      "polite\n",
      "sexual\n",
      "shit\n",
      "ahead\n",
      "rican\n",
      "places\n",
      "location\n",
      "matter\n",
      "release\n",
      "nude\n",
      "build\n",
      "fresh\n",
      "prefer\n",
      "thug\n",
      "saying\n",
      "educated\n",
      "driving\n",
      "order\n",
      "themed\n",
      "handsome\n",
      "rock\n",
      "hour\n",
      "NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**\n",
      "what\n",
      "professional\n",
      "life\n",
      "attractive\n",
      "time\n",
      "our\n",
      "platonic\n",
      "really\n",
      "dont\n",
      "chat\n",
      "honest\n",
      "girls\n",
      "its\n",
      "aussiebum\n",
      "friend\n",
      "over\n",
      "don\n",
      "text\n",
      "person\n",
      "straight\n",
      "one\n",
      "fit\n",
      "seeing\n",
      "travel\n",
      "real\n",
      "size\n",
      "about\n",
      "myday\n",
      "woman\n",
      "your\n",
      "craigslist\n",
      "well\n",
      "gent\n",
      "because\n",
      "chill\n",
      "massages\n",
      "all\n",
      "briefs\n",
      "m4w\n",
      "private\n",
      "into\n",
      "most\n",
      "could\n",
      "day\n",
      "long\n",
      "age\n",
      "relationships\n",
      "movies\n",
      "respectful\n",
      "buddy\n",
      "young\n",
      "often\n",
      "yours\n",
      "gay\n",
      "judge\n",
      "clip\n",
      "talking\n",
      "clo\n",
      "race\n",
      "always\n",
      "lots\n",
      "down\n",
      "missing\n",
      "mid\n",
      "years\n",
      "earth\n",
      "back\n",
      "backgrounds\n",
      "any\n",
      "working\n",
      "japanese\n",
      "upon\n",
      "parlor\n",
      "work\n",
      "doing\n",
      "dressed\n",
      "today\n",
      "point\n",
      "music\n",
      "accountant\n",
      "ages\n",
      "religion\n",
      "swm\n",
      "cuddler\n",
      "freaky\n",
      "oil\n",
      "underwear\n",
      "everyone\n",
      "random\n",
      "happy\n",
      "messages\n",
      "also\n",
      "date\n",
      "putting\n",
      "check\n",
      "small\n",
      "everything\n",
      "broke\n",
      "open\n",
      "week\n",
      "discreet\n",
      "drop\n",
      "offended\n",
      "felt\n",
      "special\n",
      "will\n",
      "banana\n",
      "guys\n",
      "friends\n",
      "brk\n",
      "employed\n",
      "netflix\n",
      "how\n",
      "section\n",
      "each\n",
      "during\n",
      "contact\n",
      "latinos\n",
      "plz\n",
      "newyork\n",
      "black\n",
      "hey\n",
      "laid\n",
      "seeking\n",
      "sure\n",
      "towards\n",
      "good\n",
      "weekday\n",
      "please\n",
      "massage\n",
      "attached\n",
      "com\n",
      "hangout\n",
      "thought\n",
      "own\n",
      "enjoy\n",
      "hits\n",
      "bikini\n",
      "guy\n",
      "offered\n",
      "hair\n",
      "choi\n",
      "underground\n",
      "indefinite\n",
      "fine\n",
      "more\n",
      "available\n",
      "welcome\n",
      "other\n",
      "successfully\n",
      "simultaneously\n",
      "watching\n",
      "sink\n",
      "apply\n",
      "match\n",
      "exact\n",
      "fwb\n",
      "dates\n",
      "expected\n",
      "old\n",
      "https\n",
      "personal\n",
      "puertoriceña\n",
      "body\n",
      "strictly\n",
      "nyc\n",
      "members\n",
      "separate\n",
      "rather\n",
      "nights\n",
      "midtown\n",
      "share\n",
      "maybe\n",
      "payments\n",
      "after\n",
      "year\n",
      "curse\n",
      "guess\n",
      "line\n",
      "cuddling\n",
      "drink\n",
      "final\n",
      "smoke\n",
      "night\n",
      "baths\n",
      "marry\n",
      "send\n",
      "org\n",
      "ask\n",
      "cool\n",
      "dot\n",
      "his\n",
      "reply\n",
      "nothingness\n",
      "female\n",
      "movie\n",
      "bounce\n",
      "last\n",
      "hugs\n",
      "minimum\n",
      "replies\n",
      "breast\n",
      "nice\n",
      "around\n",
      "website\n",
      "myself\n",
      "friendly\n",
      "kik\n",
      "intelligent\n",
      "html\n",
      "confused\n",
      "subwrestlingclub\n",
      "women\n",
      "looks\n",
      "them\n",
      "full\n",
      "shit\n",
      "places\n",
      "ever\n",
      "matter\n",
      "prefer\n",
      "club\n",
      "saying\n",
      "educated\n",
      "order\n",
      "rock\n",
      "hour\n",
      "hang\n",
      "brainchild\n",
      "ordained\n",
      "turns\n",
      "usually\n",
      "exotic\n",
      "anastasia\n",
      "hands\n",
      "tubs\n",
      "light\n",
      "put\n",
      "exchange\n",
      "offer\n",
      "affection\n",
      "6989\n",
      "917\n",
      "shook\n",
      "intended\n",
      "asian\n",
      "worries\n",
      "where\n",
      "pained\n",
      "lighting\n",
      "events\n",
      "cam\n",
      "early\n",
      "let\n",
      "anything\n",
      "days\n",
      "pleasurable\n",
      "foolishness\n",
      "healer\n",
      "modeling\n",
      "much\n",
      "tangible\n",
      "visiting\n",
      "mentally\n",
      "helpful\n",
      "had\n",
      "keeping\n",
      "alone\n",
      "deal\n",
      "hackney\n",
      "trying\n",
      "expectations\n",
      "younger\n",
      "sexy\n",
      "fall\n",
      "fun\n",
      "hoping\n",
      "sched\n",
      "interested\n",
      "concert\n",
      "still\n",
      "questions\n",
      "1967\n",
      "having\n",
      "glass\n",
      "nearby\n",
      "monterey\n",
      "tree\n",
      "suck\n",
      "distress\n",
      "comfortable\n",
      "pics\n",
      "those\n",
      "place\n",
      "cuddle\n",
      "anyone\n",
      "dresses\n",
      "situation\n",
      "head\n",
      "communication\n",
      "spent\n",
      "motivation\n",
      "coffee\n",
      "parkinson\n",
      "includes\n",
      "sincere\n",
      "0553\n",
      "pic\n",
      "lighter\n",
      "student\n",
      "gather\n",
      "dim\n",
      "middle\n",
      "big\n",
      "assets\n",
      "paul\n",
      "becomes\n",
      "college\n",
      "take\n",
      "host\n",
      "distance\n",
      "functional\n",
      "something\n",
      "thursday\n",
      "learning\n",
      "occasion\n",
      "552\n",
      "held\n",
      "articles\n",
      "title\n",
      "xoxo\n",
      "else\n",
      "tissue\n",
      "going\n",
      "hanging\n",
      "moments\n",
      "took\n",
      "easy\n",
      "gym\n",
      "pretty\n",
      "different\n",
      "incidents\n",
      "people\n",
      "conversationalist\n",
      "try\n",
      "subject\n",
      "split\n",
      "play\n",
      "getting\n",
      "games\n",
      "recent\n",
      "schedule\n",
      "ride\n",
      "kind\n",
      "room\n",
      "give\n",
      "towels\n",
      "tall\n",
      "route\n",
      "come\n",
      "relate\n",
      "hardships\n",
      "undies\n",
      "sucks\n",
      "minister\n",
      "therapist\n",
      "hangups\n",
      "touches\n",
      "activities\n",
      "started\n",
      "relationship\n",
      "tell\n",
      "married\n",
      "together\n",
      "bit\n",
      "her\n",
      "generous\n",
      "strip\n",
      "conversation\n",
      "ddf\n",
      "image\n",
      "touch\n",
      "three\n",
      "lift\n",
      "hot\n",
      "sensual\n",
      "dinner\n",
      "lie\n",
      "surmount\n",
      "mood\n",
      "stimulation\n",
      "california\n",
      "first\n",
      "wondering\n",
      "rosa\n",
      "feel\n",
      "yourself\n",
      "convenient\n",
      "unhurried\n",
      "lips\n",
      "cultivate\n",
      "420\n",
      "thing\n",
      "fellas\n",
      "named\n",
      "jazmine\n",
      "enjoys\n",
      "lets\n",
      "718\n",
      "hold\n",
      "361\n",
      "now\n",
      "creative\n",
      "somebody\n",
      "advise\n",
      "yes\n",
      "per\n",
      "makes\n",
      "past\n",
      "foot\n",
      "care\n",
      "wants\n",
      "serious\n",
      "routine\n",
      "did\n",
      "glen\n",
      "ring\n",
      "hard\n",
      "odds\n",
      "know\n",
      "connectedness\n",
      "note\n",
      "emotionally\n",
      "cut\n",
      "party\n",
      "variety\n",
      "again\n",
      "cannabis\n",
      "suspect\n",
      "10am\n",
      "pony\n",
      "weekend\n",
      "many\n",
      "busy\n",
      "diamond\n",
      "lately\n",
      "six\n",
      "possible\n",
      "tendril\n",
      "fabric\n",
      "got\n",
      "email\n",
      "sane\n",
      "winter\n",
      "trim\n",
      "smart\n",
      "hear\n",
      "watch\n",
      "town\n",
      "adventurous\n",
      "great\n",
      "takes\n",
      "chemistry\n",
      "relaxing\n",
      "hire\n",
      "healing\n",
      "off\n",
      "train\n",
      "topics\n",
      "write\n",
      "entry\n",
      "lean\n",
      "things\n",
      "dinners\n",
      "buddies\n",
      "otherwise\n",
      "interest\n",
      "mutually\n",
      "forgive\n",
      "partners\n",
      "regular\n",
      "athletic\n",
      "normal\n",
      "wine\n",
      "stress\n",
      "canal\n",
      "muscular\n",
      "bed\n",
      "difference\n",
      "few\n",
      "crazy\n",
      "monty\n",
      "company\n",
      "before\n",
      "experience\n",
      "bike\n",
      "two\n",
      "persona\n",
      "fitness\n",
      "thinking\n",
      "ideas\n",
      "perhaps\n",
      "forward\n",
      "under\n",
      "campos\n",
      "then\n",
      "says\n",
      "way\n",
      "showing\n",
      "hotel\n",
      "emotional\n",
      "times\n",
      "against\n",
      "socialize\n",
      "picture\n",
      "compensate\n",
      "fox\n",
      "area\n",
      "jealous\n",
      "gone\n",
      "rave\n",
      "tennis\n",
      "used\n",
      "home\n",
      "cup\n",
      "nature\n",
      "recently\n",
      "planet\n",
      "slender\n",
      "has\n",
      "6pm\n",
      "physically\n",
      "passions\n",
      "non\n",
      "was\n",
      "shopping\n",
      "ready\n",
      "dancing\n",
      "sex\n",
      "magazine\n",
      "pictures\n",
      "hurt\n",
      "means\n",
      "walking\n",
      "find\n",
      "storm\n",
      "nipples\n",
      "dude\n",
      "convinced\n",
      "willow\n",
      "passionate\n",
      "prostate\n",
      "psychotherapy\n",
      "sweat\n",
      "drive\n",
      "baby\n",
      "extra\n",
      "feminine\n",
      "workout\n",
      "strong\n",
      "dismantle\n",
      "edm\n",
      "business\n",
      "food\n",
      "luv\n",
      "naked\n",
      "mon\n",
      "hello\n",
      "m4mw\n",
      "longer\n",
      "medical\n",
      "m4m\n",
      "they\n",
      "tensions\n",
      "puerto\n",
      "mare\n",
      "relax\n",
      "euph\n",
      "easier\n",
      "cash\n",
      "lunch\n",
      "elegant\n",
      "weekends\n",
      "amp\n",
      "arts\n",
      "peeking\n",
      "similar\n",
      "smooth\n",
      "plants\n",
      "happens\n",
      "excited\n",
      "sacramento\n",
      "these\n",
      "power\n",
      "youre\n",
      "enhance\n",
      "from\n",
      "even\n",
      "free\n",
      "shower\n",
      "make\n",
      "name\n",
      "deep\n",
      "best\n",
      "posting\n",
      "trini\n",
      "soak\n",
      "session\n",
      "queens\n",
      "model\n",
      "interesting\n",
      "horrible\n",
      "meeting\n",
      "eastern\n",
      "clothed\n",
      "involved\n",
      "skiers\n",
      "sub\n",
      "gentleman\n",
      "able\n",
      "shape\n",
      "explore\n",
      "mentee\n",
      "answered\n",
      "lotion\n",
      "contac\n",
      "bimwm\n",
      "etc\n",
      "didn\n",
      "possessiv\n",
      "silvia\n",
      "vers\n",
      "clean\n",
      "man\n",
      "kiss\n",
      "relaxation\n",
      "lot\n",
      "cute\n",
      "soft\n",
      "type\n",
      "executive\n",
      "toned\n",
      "money\n",
      "silky\n",
      "flaky\n",
      "legit\n",
      "hopefully\n",
      "radical\n",
      "bindrim\n",
      "frie\n",
      "little\n",
      "scale\n",
      "been\n",
      "specific\n",
      "heated\n",
      "edc\n",
      "wide\n",
      "bodywork\n",
      "advice\n",
      "live\n",
      "fee\n",
      "polite\n",
      "using\n",
      "sexual\n",
      "clear\n",
      "pay\n",
      "hug\n",
      "ahead\n",
      "rican\n",
      "location\n",
      "girl\n",
      "hiking\n",
      "release\n",
      "nude\n",
      "build\n",
      "treatment\n",
      "fresh\n",
      "possibly\n",
      "thug\n",
      "55lbs\n",
      "driving\n",
      "stuff\n",
      "flash\n",
      "themed\n",
      "handsome\n",
      "join\n",
      "break\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "getTopWords(ny,sf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
